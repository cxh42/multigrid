# PPO specific parameters
mode: "ppo"
learning_rate: 2.5e-4
total_timesteps: 500000
num_envs: 4
num_steps: 128
gamma: 0.99
gae_lambda: 0.95
num_minibatches: 4
update_epochs: 4
norm_adv: true
clip_coef: 0.2
clip_vloss: true
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
anneal_lr: true