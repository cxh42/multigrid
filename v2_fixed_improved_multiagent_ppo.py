"""
ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOå®ç° - è§£å†³entropy_coefå¡ä½å’Œæ€§èƒ½ä¸‹é™é—®é¢˜

ä¸»è¦ä¿®å¤ï¼š
1. entropy_coefä¸Šé™ä»0.1æé«˜åˆ°0.3
2. åœæ»æ£€æµ‹é¢‘ç‡ä»100è½®é™ä½åˆ°300è½®  
3. ç§»é™¤ç ´åæ€§å‚æ•°å™ªå£°
4. æ·»åŠ æ™ºèƒ½æ¢å¤ç­–ç•¥
5. æ¨¡å‹ä¿å­˜åˆ°model2æ–‡ä»¶å¤¹
6. æ›´ä¿å®ˆçš„è°ƒæ•´ç­–ç•¥
"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions.categorical import Categorical
import argparse
import matplotlib.pyplot as plt
import os
from collections import defaultdict, deque
import time
import json
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ç¯å¢ƒå¯¼å…¥
from envs import gym_multigrid
from envs.gym_multigrid import multigrid_envs

# WandBæ”¯æŒ
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not available, using local logging only")


class RewardShaper:
    """å¥–åŠ±å¡‘å½¢å™¨ - è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜"""
    
    def __init__(self, n_agents, grid_size=15):
        self.n_agents = n_agents
        self.grid_size = grid_size
        self.reset()
        
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.prev_positions = [None] * self.n_agents
        self.visited_positions = [set() for _ in range(self.n_agents)]
        self.step_count = 0
        self.prev_distances = [None] * self.n_agents
        self.consecutive_stationary = [0] * self.n_agents
        
    def get_goal_position(self, env):
        """è·å–ç›®æ ‡ä½ç½®"""
        try:
            for i in range(env.width):
                for j in range(env.height):
                    cell = env.grid.get(i, j)
                    if cell and cell.type == 'goal':
                        return (i, j)
            return (13, 13)  # é»˜è®¤ä½ç½®
        except:
            return (13, 13)
    
    def shape_rewards(self, env, agent_positions, original_rewards, actions):
        """å¥–åŠ±å¡‘å½¢ä¸»å‡½æ•°"""
        shaped_rewards = list(original_rewards)
        goal_pos = self.get_goal_position(env)
        
        for i in range(self.n_agents):
            pos = tuple(agent_positions[i])
            action = actions[i]
            
            # 1. æ¢ç´¢å¥–åŠ±
            if pos not in self.visited_positions[i]:
                shaped_rewards[i] += 0.1
                self.visited_positions[i].add(pos)
                
            # 2. ç§»åŠ¨å¥–åŠ±
            if action == 2:  # forward
                shaped_rewards[i] += 0.03
                self.consecutive_stationary[i] = 0
            elif action in [0, 1]:  # left, right
                shaped_rewards[i] += 0.01
                self.consecutive_stationary[i] = 0
            else:
                self.consecutive_stationary[i] += 1
                
            # 3. åé™æ­¢æƒ©ç½š
            if self.consecutive_stationary[i] > 3:
                shaped_rewards[i] -= 0.02 * (self.consecutive_stationary[i] - 3)
                
            # 4. ç›®æ ‡å¯¼å‘å¥–åŠ±
            current_dist = np.linalg.norm(np.array(pos) - np.array(goal_pos))
            if self.prev_distances[i] is not None:
                dist_change = self.prev_distances[i] - current_dist
                shaped_rewards[i] += dist_change * 0.02
                
            self.prev_distances[i] = current_dist
            
            # 5. æ—¶é—´å‹åŠ›
            shaped_rewards[i] -= 0.005
            
        self.step_count += 1
        return shaped_rewards


class BehaviorMonitor:
    """è¡Œä¸ºç›‘æ§å™¨"""
    
    def __init__(self, n_agents, window_size=100):
        self.n_agents = n_agents
        self.window_size = window_size
        self.reset()
        
    def reset(self):
        """é‡ç½®ç›‘æ§çŠ¶æ€"""
        self.action_history = [deque(maxlen=self.window_size) for _ in range(self.n_agents)]
        self.position_history = [deque(maxlen=self.window_size) for _ in range(self.n_agents)]
        self.step_count = 0
        
    def update(self, actions, positions):
        """æ›´æ–°ç›‘æ§æ•°æ®"""
        for i in range(self.n_agents):
            self.action_history[i].append(actions[i])
            self.position_history[i].append(tuple(positions[i]))
        self.step_count += 1
        
    def get_activity_metrics(self):
        """è®¡ç®—æ´»è·ƒåº¦æŒ‡æ ‡"""
        if self.step_count < 10:
            return {}
            
        metrics = {}
        for i in range(self.n_agents):
            if len(self.action_history[i]) == 0:
                continue
                
            # ç§»åŠ¨é¢‘ç‡
            move_actions = [0, 1, 2]
            recent_actions = list(self.action_history[i])[-50:]
            move_ratio = sum(1 for a in recent_actions if a in move_actions) / len(recent_actions)
            
            # æ¢ç´¢èŒƒå›´
            recent_positions = list(self.position_history[i])[-50:]
            unique_positions = len(set(recent_positions))
            
            # ä½ç½®å˜åŒ–é¢‘ç‡
            position_changes = sum(1 for j in range(1, len(recent_positions)) 
                                 if recent_positions[j] != recent_positions[j-1])
            change_ratio = position_changes / max(1, len(recent_positions) - 1)
            
            metrics[f'agent_{i}_move_ratio'] = move_ratio
            metrics[f'agent_{i}_unique_positions'] = unique_positions
            metrics[f'agent_{i}_position_change_ratio'] = change_ratio
            
        # æ•´ä½“æŒ‡æ ‡
        if metrics:
            avg_move_ratio = np.mean([v for k, v in metrics.items() if 'move_ratio' in k])
            avg_exploration = np.mean([v for k, v in metrics.items() if 'unique_positions' in k])
            metrics['avg_move_ratio'] = avg_move_ratio
            metrics['avg_exploration'] = avg_exploration
            
        return metrics


class ImprovedMultiGridPPOAgent(nn.Module):
    """æ”¹è¿›çš„PPOæ™ºèƒ½ä½“ç½‘ç»œ"""
    
    def __init__(self, n_actions=7):
        super().__init__()
        
        # å›¾åƒå¤„ç†ç½‘ç»œ
        self.image_conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1), 
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        # æ–¹å‘åµŒå…¥
        self.direction_embed = nn.Embedding(4, 16)
        
        # å…±äº«ç‰¹å¾ç½‘ç»œ
        self.shared = nn.Sequential(
            nn.Linear(64 + 16, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
        )
        
        # Actorå’ŒCriticå¤´
        self.actor = nn.Linear(128, n_actions)
        self.critic = nn.Linear(128, 1)
        
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, 0.5)
            module.bias.data.fill_(0.0)
        elif isinstance(module, nn.Conv2d):
            torch.nn.init.orthogonal_(module.weight, 1.0)
            
    def forward(self, obs):
        """å‰å‘ä¼ æ’­"""
        image = obs['image']
        direction = obs['direction']
        
        # å¤„ç†å›¾åƒæ ¼å¼
        if len(image.shape) == 4:
            image = image.permute(0, 3, 1, 2)
        elif len(image.shape) == 3:
            image = image.permute(2, 0, 1).unsqueeze(0)
        
        # å¤„ç†æ–¹å‘æ ¼å¼
        if len(direction.shape) > 1:
            direction = direction.squeeze(-1)
        if len(direction.shape) == 0:
            direction = direction.unsqueeze(0)
        
        # ç‰¹å¾æå–
        image_features = self.image_conv(image.float())
        direction_features = self.direction_embed(direction.long())
        
        # ç¡®ä¿batchç»´åº¦åŒ¹é…
        batch_size = image_features.shape[0]
        if direction_features.shape[0] != batch_size:
            if direction_features.shape[0] == 1:
                direction_features = direction_features.repeat(batch_size, 1)
        
        # åˆå¹¶ç‰¹å¾
        combined = torch.cat([image_features, direction_features], dim=-1)
        shared_features = self.shared(combined)
        
        # è¾“å‡º
        action_logits = self.actor(shared_features)
        value = self.critic(shared_features)
        
        return action_logits, value.squeeze(-1)
    
    def get_action_and_value(self, obs, action=None):
        """è·å–åŠ¨ä½œå’Œä»·å€¼"""
        action_logits, value = self.forward(obs)
        dist = Categorical(logits=action_logits)
        
        if action is None:
            action = dist.sample()
        
        return action, dist.log_prob(action), dist.entropy(), value


class FixedMultiAgentPPOController:
    """ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOæ§åˆ¶å™¨"""
    
    def __init__(self, env_name, n_agents, device, lr=2e-4, n_parallel_envs=4):
        self.env_name = env_name
        self.n_agents = n_agents
        self.device = device
        self.n_parallel_envs = n_parallel_envs
        
        # åˆ›å»ºæ™ºèƒ½ä½“å’Œä¼˜åŒ–å™¨
        self.agents = []
        self.optimizers = []
        
        for i in range(n_agents):
            agent = ImprovedMultiGridPPOAgent(n_actions=7).to(device)
            optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)
            
            self.agents.append(agent)
            self.optimizers.append(optimizer)
        
        # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        self.envs = self._create_parallel_envs()
        
        # å¥–åŠ±å¡‘å½¢å’Œè¡Œä¸ºç›‘æ§
        self.reward_shapers = [RewardShaper(n_agents) for _ in range(n_parallel_envs)]
        self.behavior_monitor = BehaviorMonitor(n_agents)
        
        # ä¿®å¤åçš„è‡ªé€‚åº”è®­ç»ƒå‚æ•°
        self.entropy_coef = 0.05  # åˆå§‹å€¼
        self.initial_entropy_coef = 0.05  # è®°å½•åˆå§‹å€¼
        self.performance_history = deque(maxlen=2000)  # å¢åŠ å†å²é•¿åº¦
        self.stagnation_count = 0
        self.last_adjustment_episode = 0  # è®°å½•ä¸Šæ¬¡è°ƒæ•´çš„episode
        self.best_performance = float('-inf')
        self.best_performance_entropy = 0.05
        
        self.reset_buffers()
        
        print(f"âœ… åˆ›å»ºäº† {n_agents} ä¸ªä¿®å¤ç‰ˆPPOæ™ºèƒ½ä½“")
        print(f"âœ… ä½¿ç”¨ {n_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")
        print(f"âœ… ä¿®å¤åœæ»æ£€æµ‹æœºåˆ¶ï¼Œæ¨¡å‹ä¿å­˜åˆ°model2/")
        print(f"âœ… è®¾å¤‡: {device}")
    
    def _create_parallel_envs(self):
        """åˆ›å»ºå¹¶è¡Œç¯å¢ƒ"""
        envs = []
        for i in range(self.n_parallel_envs):
            env = gym.make(self.env_name)
            envs.append(env)
        return envs
    
    def reset_buffers(self):
        """é‡ç½®ç»éªŒç¼“å†²åŒº"""
        self.buffers = []
        for i in range(self.n_agents):
            self.buffers.append({
                'observations': [],
                'actions': [],
                'log_probs': [],
                'values': [],
                'rewards': [],
                'dones': []
            })
    
    def get_actions(self, obs):
        """è·å–å•ç¯å¢ƒçš„åŠ¨ä½œ"""
        agent_observations = []
        
        for i in range(self.n_agents):
            agent_obs = {
                'image': torch.FloatTensor(obs['image'][i]).to(self.device),
                'direction': torch.LongTensor([obs['direction'][i]]).to(self.device)
            }
            agent_observations.append(agent_obs)
        
        actions = []
        log_probs = []
        values = []
        entropies = []
        
        for i in range(self.n_agents):
            with torch.no_grad():
                action, log_prob, entropy, value = self.agents[i].get_action_and_value(agent_observations[i])
            
            actions.append(action.item())
            log_probs.append(log_prob.item())
            values.append(value.item())
            entropies.append(entropy.item())
        
        return actions, log_probs, values, entropies, agent_observations
    
    def get_parallel_actions(self, obs_list):
        """è·å–å¹¶è¡Œç¯å¢ƒçš„åŠ¨ä½œ"""
        batched_agent_obs, individual_obs = self.preprocess_parallel_obs(obs_list)
        
        all_actions = []
        all_log_probs = []
        all_values = []
        all_entropies = []
        
        for i in range(self.n_agents):
            if batched_agent_obs[i] is not None:
                with torch.no_grad():
                    actions, log_probs, entropies, values = self.agents[i].get_action_and_value(batched_agent_obs[i])
                
                all_actions.append(actions.cpu().numpy())
                all_log_probs.append(log_probs.cpu().numpy())
                all_values.append(values.cpu().numpy())
                all_entropies.append(entropies.cpu().numpy())
            else:
                all_actions.append(np.array([]))
                all_log_probs.append(np.array([]))
                all_values.append(np.array([]))
                all_entropies.append(np.array([]))
        
        # é‡æ–°ç»„ç»‡ä¸ºæ¯ä¸ªç¯å¢ƒçš„åŠ¨ä½œåˆ—è¡¨
        env_actions = []
        for env_idx in range(self.n_parallel_envs):
            env_action = []
            for agent_idx in range(self.n_agents):
                if len(all_actions[agent_idx]) > env_idx:
                    env_action.append(all_actions[agent_idx][env_idx])
                else:
                    env_action.append(0)
            env_actions.append(env_action)
        
        return env_actions, all_log_probs, all_values, all_entropies, individual_obs
    
    def preprocess_parallel_obs(self, obs_list):
        """é¢„å¤„ç†å¹¶è¡Œè§‚å¯Ÿ"""
        agent_observations = [[] for _ in range(self.n_agents)]
        
        for env_obs in obs_list:
            for i in range(self.n_agents):
                agent_obs = {
                    'image': torch.FloatTensor(env_obs['image'][i]).to(self.device),
                    'direction': torch.LongTensor([env_obs['direction'][i]]).to(self.device)
                }
                agent_observations[i].append(agent_obs)
        
        # æ‰¹å¤„ç†è§‚å¯Ÿ
        batched_agent_obs = []
        for i in range(self.n_agents):
            if len(agent_observations[i]) > 0:
                batch_images = torch.stack([obs['image'] for obs in agent_observations[i]])
                batch_directions = torch.stack([obs['direction'] for obs in agent_observations[i]])
                batch_directions = batch_directions.squeeze(-1)
                
                batched_obs = {
                    'image': batch_images,
                    'direction': batch_directions
                }
                batched_agent_obs.append(batched_obs)
            else:
                batched_agent_obs.append(None)
        
        return batched_agent_obs, agent_observations
    
    def collect_parallel_rollout(self, n_steps=256):
        """æ”¶é›†å¸¦å¥–åŠ±å¡‘å½¢çš„ç»éªŒ"""
        self.reset_buffers()
        
        # é‡ç½®ç¯å¢ƒå’Œå¥–åŠ±å¡‘å½¢å™¨
        obs_list = []
        for i, env in enumerate(self.envs):
            obs = env.reset()
            obs_list.append(obs)
            self.reward_shapers[i].reset()
        
        episode_actions = []
        episode_positions = []
        
        for step in range(n_steps):
            # è·å–åŠ¨ä½œ
            env_actions, log_probs_list, values_list, entropies_list, individual_obs = self.get_parallel_actions(obs_list)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs_list = []
            rewards_list = []
            dones_list = []
            
            for env_idx, env in enumerate(self.envs):
                next_obs, rewards, done, info = env.step(env_actions[env_idx])
                
                # å¥–åŠ±å¡‘å½¢
                agent_positions = [env.agent_pos[i] for i in range(self.n_agents)]
                shaped_rewards = self.reward_shapers[env_idx].shape_rewards(
                    env, agent_positions, rewards, env_actions[env_idx])
                
                next_obs_list.append(next_obs)
                rewards_list.append(shaped_rewards)
                dones_list.append(done)
                
                # æ”¶é›†è¡Œä¸ºæ•°æ®
                episode_actions.extend(env_actions[env_idx])
                episode_positions.extend(agent_positions)
            
            # å­˜å‚¨ç»éªŒ
            for env_idx in range(self.n_parallel_envs):
                for agent_idx in range(self.n_agents):
                    if len(individual_obs[agent_idx]) > env_idx:
                        self.buffers[agent_idx]['observations'].append(individual_obs[agent_idx][env_idx])
                        self.buffers[agent_idx]['actions'].append(env_actions[env_idx][agent_idx])
                        
                        if len(log_probs_list[agent_idx]) > env_idx:
                            self.buffers[agent_idx]['log_probs'].append(log_probs_list[agent_idx][env_idx])
                            self.buffers[agent_idx]['values'].append(values_list[agent_idx][env_idx])
                        else:
                            self.buffers[agent_idx]['log_probs'].append(0.0)
                            self.buffers[agent_idx]['values'].append(0.0)
                        
                        self.buffers[agent_idx]['rewards'].append(rewards_list[env_idx][agent_idx])
                        self.buffers[agent_idx]['dones'].append(dones_list[env_idx])
            
            # é‡ç½®å®Œæˆçš„ç¯å¢ƒ
            for env_idx, done in enumerate(dones_list):
                if done:
                    obs_list[env_idx] = self.envs[env_idx].reset()
                    self.reward_shapers[env_idx].reset()
                else:
                    obs_list[env_idx] = next_obs_list[env_idx]
        
        # æ›´æ–°è¡Œä¸ºç›‘æ§
        if episode_actions and episode_positions:
            step_actions = [episode_actions[i:i+self.n_agents] for i in range(0, len(episode_actions), self.n_agents)]
            step_positions = [episode_positions[i:i+self.n_agents] for i in range(0, len(episode_positions), self.n_agents)]
            
            for actions, positions in zip(step_actions[:50], step_positions[:50]):
                self.behavior_monitor.update(actions, positions)
        
        return self.buffers
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡"""
        advantages = []
        returns = []
        
        advantage = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            next_non_terminal = 1 - dones[t]
            delta = rewards[t] + gamma * next_value * next_non_terminal - values[t]
            advantage = delta + gamma * gae_lambda * next_non_terminal * advantage
            
            advantages.insert(0, advantage)
            returns.insert(0, advantage + values[t])
        
        return advantages, returns
    
    def update_agent(self, agent_id, n_epochs=4, batch_size=64, clip_coef=0.2):
        """æ›´æ–°å•ä¸ªæ™ºèƒ½ä½“"""
        buffer = self.buffers[agent_id]
        agent = self.agents[agent_id]
        optimizer = self.optimizers[agent_id]
        
        if len(buffer['rewards']) == 0:
            return {}
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages, returns = self.compute_gae(
            buffer['rewards'], buffer['values'], buffer['dones'])
        
        # è½¬æ¢ä¸ºå¼ é‡
        observations = buffer['observations']
        actions = torch.LongTensor(buffer['actions']).to(self.device)
        old_log_probs = torch.FloatTensor(buffer['log_probs']).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
        
        # PPOæ›´æ–°
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        
        for epoch in range(n_epochs):
            indices = torch.randperm(len(observations))
            
            for start in range(0, len(observations), batch_size):
                end = min(start + batch_size, len(observations))
                batch_indices = indices[start:end]
                
                # å‡†å¤‡æ‰¹æ¬¡
                batch_images = torch.stack([observations[i]['image'] for i in batch_indices])
                batch_directions = torch.stack([observations[i]['direction'] for i in batch_indices])
                batch_directions = batch_directions.squeeze(-1)
                
                batch_obs = {
                    'image': batch_images,
                    'direction': batch_directions
                }
                
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages_tensor[batch_indices]
                batch_returns = returns_tensor[batch_indices]
                
                # å‰å‘ä¼ æ’­
                _, new_log_probs, entropy, new_values = agent.get_action_and_value(batch_obs, batch_actions)
                
                # è®¡ç®—æŸå¤±
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                value_loss = 0.5 * (new_values - batch_returns).pow(2).mean()
                entropy_loss = -self.entropy_coef * entropy.mean()
                
                total_loss = policy_loss + value_loss + entropy_loss
                
                # æ›´æ–°
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)
                optimizer.step()
                
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy_loss += entropy_loss.item()
        
        return {
            'policy_loss': total_policy_loss / (n_epochs * len(range(0, len(observations), batch_size))),
            'value_loss': total_value_loss / (n_epochs * len(range(0, len(observations), batch_size))),
            'entropy_loss': total_entropy_loss / (n_epochs * len(range(0, len(observations), batch_size)))
        }
    
    def update_all_agents(self):
        """æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“"""
        metrics = {}
        for i in range(self.n_agents):
            agent_metrics = self.update_agent(i)
            for key, value in agent_metrics.items():
                if key not in metrics:
                    metrics[key] = []
                metrics[key].append(value)
        
        # å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key, values in metrics.items():
            if values:
                avg_metrics[f'avg_{key}'] = np.mean(values)
        
        return avg_metrics
    
    def check_and_handle_stagnation(self, current_reward, episode):
        """ä¿®å¤ç‰ˆåœæ»æ£€æµ‹å’Œå¤„ç†"""
        self.performance_history.append(current_reward)
        
        # è®°å½•æœ€ä½³æ€§èƒ½
        if current_reward > self.best_performance:
            self.best_performance = current_reward
            self.best_performance_entropy = self.entropy_coef
            print(f"ğŸ† æ–°çš„æœ€ä½³æ€§èƒ½: {current_reward:.2f} (entropy_coef: {self.entropy_coef:.3f})")
        
        if len(self.performance_history) < 300:  # éœ€è¦æ›´å¤šæ•°æ®
            return False
        
        # ä½¿ç”¨æ›´é•¿çš„æ—¶é—´çª—å£æ£€æŸ¥åœæ»
        recent_avg = np.mean(list(self.performance_history)[-100:])  # æœ€è¿‘100è½®
        older_avg = np.mean(list(self.performance_history)[-300:-200])  # æ›´æ—©çš„100è½®
        
        # æ›´ä¸¥æ ¼çš„åœæ»æ¡ä»¶
        performance_decline = recent_avg < older_avg * 0.95  # ä¸‹é™è¶…è¿‡5%
        
        if performance_decline:
            self.stagnation_count += 1
        else:
            self.stagnation_count = 0
        
        # åªæœ‰æŒç»­åœæ»300è½®ä¸”è·ç¦»ä¸Šæ¬¡è°ƒæ•´è‡³å°‘200è½®æ‰è§¦å‘
        episodes_since_adjustment = episode - self.last_adjustment_episode
        
        if (self.stagnation_count > 300 and episodes_since_adjustment > 200):
            print(f"âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡è®­ç»ƒåœæ» (åœæ»{self.stagnation_count}è½®)")
            print(f"   å½“å‰æ€§èƒ½: {recent_avg:.2f} vs å†å²: {older_avg:.2f}")
            
            # æ™ºèƒ½æ¢å¤ç­–ç•¥
            self._smart_recovery_strategy(recent_avg, older_avg, episode)
            
            self.stagnation_count = 0
            self.last_adjustment_episode = episode
            return True
        
        return False
    
    def _smart_recovery_strategy(self, recent_avg, older_avg, episode):
        """æ™ºèƒ½æ¢å¤ç­–ç•¥"""
        performance_ratio = recent_avg / older_avg
        
        if performance_ratio < 0.8:  # ä¸¥é‡ä¸‹é™
            print("ğŸš¨ ä¸¥é‡æ€§èƒ½ä¸‹é™ï¼Œæ‰§è¡Œä¿å®ˆæ¢å¤...")
            
            # å›åˆ°æœ€ä½³æ€§èƒ½æ—¶çš„entropy_coef
            if self.best_performance_entropy != self.entropy_coef:
                old_entropy = self.entropy_coef
                self.entropy_coef = self.best_performance_entropy
                print(f"ğŸ”§ æ¢å¤æœ€ä½³entropy_coef: {old_entropy:.3f} â†’ {self.entropy_coef:.3f}")
            
            # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ä¿å®ˆçš„å­¦ä¹ 
            for i, optimizer in enumerate(self.optimizers):
                for param_group in optimizer.param_groups:
                    old_lr = param_group['lr']
                    param_group['lr'] = max(1e-5, old_lr * 0.8)
                    print(f"Agent {i} å­¦ä¹ ç‡: {old_lr:.6f} â†’ {param_group['lr']:.6f}")
                    
        elif performance_ratio < 0.9:  # ä¸­ç­‰ä¸‹é™
            print("âš ï¸ ä¸­ç­‰æ€§èƒ½ä¸‹é™ï¼Œæ¸©å’Œè°ƒæ•´...")
            
            # ç¨å¾®å¢åŠ entropy_coefï¼Œä½†æœ‰ä¸Šé™
            old_entropy = self.entropy_coef
            self.entropy_coef = min(0.3, self.entropy_coef * 1.1)  # âœ… ä¿®å¤ï¼šä¸Šé™æé«˜åˆ°0.3
            print(f"ğŸ”§ æ¸©å’Œå¢åŠ entropy_coef: {old_entropy:.3f} â†’ {self.entropy_coef:.3f}")
            
        else:  # è½»å¾®ä¸‹é™
            print("ğŸ“Š è½»å¾®æ€§èƒ½æ³¢åŠ¨ï¼Œä¿æŒç›‘æ§...")
            
            # åªæ˜¯ç¨å¾®è°ƒæ•´ï¼Œä¸åšå¤§æ”¹åŠ¨
            if self.entropy_coef < 0.08:
                self.entropy_coef = min(0.15, self.entropy_coef * 1.05)
                print(f"ğŸ”§ å¾®è°ƒentropy_coef: {self.entropy_coef:.3f}")
        
        # é‡ç½®å¥–åŠ±å¡‘å½¢å™¨ï¼Œå¯èƒ½å¥–åŠ±å¡‘å½¢åœ¨æŸä¸ªé˜¶æ®µå¤±æ•ˆ
        for shaper in self.reward_shapers:
            shaper.reset()
        
        print("âœ… æ™ºèƒ½æ¢å¤ç­–ç•¥æ‰§è¡Œå®Œæˆ")
    
    def emergency_reset(self):
        """ç´§æ€¥é‡ç½®åŠŸèƒ½ - å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰‹åŠ¨è°ƒç”¨"""
        print("ğŸš‘ æ‰§è¡Œç´§æ€¥é‡ç½®...")
        
        # æ¢å¤åˆ°ç»è¿‡éªŒè¯çš„ç¨³å®šå€¼
        self.entropy_coef = 0.08
        self.stagnation_count = 0
        self.last_adjustment_episode = 0
        
        # é‡ç½®å­¦ä¹ ç‡åˆ°åˆå§‹å€¼
        for optimizer in self.optimizers:
            for param_group in optimizer.param_groups:
                param_group['lr'] = 2e-4
        
        # æ¸…ç©ºæ€§èƒ½å†å²ï¼Œé‡æ–°å¼€å§‹ç›‘æ§
        self.performance_history.clear()
        
        print("âœ… ç´§æ€¥é‡ç½®å®Œæˆï¼Œåº”è¯¥èƒ½åœæ­¢æ€§èƒ½ä¸‹é™")
    
    def save_models(self, path_prefix):
        """ä¿å­˜æ¨¡å‹åˆ°model2æ–‡ä»¶å¤¹"""
        # ç¡®ä¿è·¯å¾„åŒ…å«model2
        if "model2" not in path_prefix:
            path_prefix = path_prefix.replace("models/", "model2/")
        
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else "model2", exist_ok=True)
        for i in range(self.n_agents):
            path = f"{path_prefix}_agent_{i}.pth"
            torch.save(self.agents[i].state_dict(), path)
        print(f"âœ… å·²ä¿å­˜ {self.n_agents} ä¸ªæ™ºèƒ½ä½“æ¨¡å‹åˆ°: {path_prefix}_agent_*.pth")
    
    def load_models(self, path_prefix):
        """ä»model2æ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹"""
        if "model2" not in path_prefix:
            path_prefix = path_prefix.replace("models/", "model2/")
            
        for i in range(self.n_agents):
            path = f"{path_prefix}_agent_{i}.pth"
            if os.path.exists(path):
                self.agents[i].load_state_dict(torch.load(path, map_location=self.device))
            else:
                print(f"Warning: æ¨¡å‹æ–‡ä»¶ {path} ä¸å­˜åœ¨")
        print(f"âœ… å·²ä»model2/åŠ è½½ {self.n_agents} ä¸ªæ™ºèƒ½ä½“æ¨¡å‹: {path_prefix}_agent_*.pth")
    
    def close_envs(self):
        """å…³é—­ç¯å¢ƒ"""
        for env in self.envs:
            env.close()


def train_fixed_multiagent_ppo(
    env_name="MultiGrid-Cluttered-Fixed-15x15",
    n_episodes=50000,
    n_steps=256,
    n_parallel_envs=4,
    use_wandb=True,
    project_name="fixed-multigrid-ppo",
    experiment_name=None
):
    """ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒå‡½æ•°"""
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("ğŸ› ï¸ å¯åŠ¨ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ")
    print("="*60)
    print("ğŸ”§ ä¸»è¦ä¿®å¤:")
    print("   âœ… entropy_coefä¸Šé™: 0.1 â†’ 0.3")
    print("   âœ… åœæ»æ£€æµ‹é¢‘ç‡: 100è½® â†’ 300è½®")
    print("   âœ… ç§»é™¤ç ´åæ€§å‚æ•°å™ªå£°")
    print("   âœ… æ™ºèƒ½æ¢å¤ç­–ç•¥")
    print("   âœ… æ¨¡å‹ä¿å­˜åˆ°model2/æ–‡ä»¶å¤¹")
    print("   âœ… æ›´ä¿å®ˆçš„è°ƒæ•´ç­–ç•¥")
    print("="*60)
    
    # è·å–ç¯å¢ƒä¿¡æ¯
    temp_env = gym.make(env_name)
    n_agents = temp_env.n_agents
    temp_env.close()
    
    print(f"ğŸ¯ ç¯å¢ƒ: {env_name}")
    print(f"ğŸ¤– æ™ºèƒ½ä½“æ•°é‡: {n_agents}")
    print(f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {n_episodes}")
    print(f"ğŸ”„ å¹¶è¡Œç¯å¢ƒ: {n_parallel_envs}")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # å®éªŒåç§°
    if experiment_name is None:
        experiment_name = f"fixed_{env_name}_{n_agents}agents_{int(time.time())}"
    
    # WandBåˆå§‹åŒ–
    if use_wandb and WANDB_AVAILABLE:
        try:
            wandb.init(
                project=project_name,
                name=experiment_name,
                config={
                    "env_name": env_name,
                    "n_agents": n_agents,
                    "n_episodes": n_episodes,
                    "n_steps": n_steps,
                    "n_parallel_envs": n_parallel_envs,
                    "device": str(device),
                    "algorithm": "Fixed Independent PPO with Smart Recovery",
                    "fixes": [
                        "entropy_upper_limit_increased",
                        "stagnation_detection_improved", 
                        "smart_recovery_strategy",
                        "removed_parameter_noise",
                        "conservative_adjustment"
                    ]
                },
                tags=["fixed", "multi-agent", "ppo", "multigrid", "stable"]
            )
            print("âœ… WandBæ—¥å¿—å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸ WandBåˆå§‹åŒ–å¤±è´¥: {e}")
            use_wandb = False
    else:
        use_wandb = False
        print("ğŸ“ ä½¿ç”¨æœ¬åœ°æ—¥å¿—")
    
    # åˆ›å»ºä¿®å¤ç‰ˆæ§åˆ¶å™¨
    controller = FixedMultiAgentPPOController(
        env_name, n_agents, device, 
        lr=2e-4,
        n_parallel_envs=n_parallel_envs
    )
    
    # è®­ç»ƒæŒ‡æ ‡
    episode_rewards = []
    collective_rewards = []
    best_collective_reward = float('-inf')
    adjustment_history = []
    
    print(f"\nğŸ¯ å¼€å§‹ä¿®å¤ç‰ˆè®­ç»ƒ...")
    start_time = time.time()
    
    for episode in range(n_episodes):
        episode_start = time.time()
        
        # æ”¶é›†ç»éªŒ
        buffers = controller.collect_parallel_rollout(n_steps)
        
        # æ›´æ–°æ™ºèƒ½ä½“
        update_metrics = controller.update_all_agents()
        
        # è®¡ç®—å¥–åŠ±
        agent_rewards = []
        for i in range(n_agents):
            agent_reward = sum(buffers[i]['rewards'])
            agent_rewards.append(agent_reward)
        
        collective_reward = sum(agent_rewards)
        episode_rewards.append(agent_rewards)
        collective_rewards.append(collective_reward)
        
        # ä¿®å¤ç‰ˆåœæ»æ£€æµ‹
        stagnation_handled = controller.check_and_handle_stagnation(collective_reward, episode)
        if stagnation_handled:
            adjustment_history.append({
                'episode': episode,
                'performance': collective_reward,
                'entropy_coef': controller.entropy_coef
            })
        
        # è·å–è¡Œä¸ºæŒ‡æ ‡
        behavior_metrics = controller.behavior_monitor.get_activity_metrics()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if collective_reward > best_collective_reward:
            best_collective_reward = collective_reward
            controller.save_models(f"model2/best_{experiment_name}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        episode_time = time.time() - episode_start
        
        if len(collective_rewards) >= 100:
            avg_collective = np.mean(collective_rewards[-100:])
            avg_individual = np.mean([np.mean([ep[i] for ep in episode_rewards[-100:]]) for i in range(n_agents)])
        else:
            avg_collective = collective_reward
            avg_individual = np.mean(agent_rewards)
        
        # WandBæ—¥å¿—
        if use_wandb:
            log_dict = {
                "episode": episode,
                "collective_reward": collective_reward,
                "avg_collective_reward_100": avg_collective,
                "avg_individual_reward_100": avg_individual,
                "best_collective_reward": best_collective_reward,
                "episode_time": episode_time,
                "total_samples": (episode + 1) * n_steps * n_parallel_envs,
                "entropy_coef": controller.entropy_coef,
                "stagnation_handled": stagnation_handled,
                "best_performance": controller.best_performance,
                "stagnation_count": controller.stagnation_count
            }
            
            # æ·»åŠ ä¸ªä½“æ™ºèƒ½ä½“å¥–åŠ±
            for i in range(n_agents):
                log_dict[f"agent_{i}_reward"] = agent_rewards[i]
            
            # æ·»åŠ è®­ç»ƒæŒ‡æ ‡
            for key, value in update_metrics.items():
                log_dict[key] = value
            
            # æ·»åŠ è¡Œä¸ºæŒ‡æ ‡
            for key, value in behavior_metrics.items():
                log_dict[f"behavior_{key}"] = value
            
            wandb.log(log_dict)
        
        # æ§åˆ¶å°è¾“å‡º
        if episode % 50 == 0:
            total_time = time.time() - start_time
            eps_per_hour = episode * 3600 / total_time if total_time > 0 else 0
            
            # è¡Œä¸ºç»Ÿè®¡
            move_ratio = behavior_metrics.get('avg_move_ratio', 0)
            exploration = behavior_metrics.get('avg_exploration', 0)
            
            print(f"Episode {episode:6d} | "
                  f"é›†ä½“å¥–åŠ±: {avg_collective:7.2f} | "
                  f"ä¸ªä½“å¹³å‡: {avg_individual:7.2f} | "
                  f"æœ€ä½³: {best_collective_reward:7.2f} | "
                  f"ç§»åŠ¨ç‡: {move_ratio:.2f} | "
                  f"æ¢ç´¢: {exploration:.1f} | "
                  f"é€Ÿåº¦: {eps_per_hour:.1f} ep/h")
            
            if controller.entropy_coef != controller.initial_entropy_coef:
                print(f"        ğŸ”§ å½“å‰entropy_coef: {controller.entropy_coef:.3f}")
            
            if stagnation_handled:
                print(f"        ğŸ”„ å·²å¤„ç†è®­ç»ƒåœæ»")
        
        # å®šæœŸä¿å­˜
        if episode % 2000 == 0 and episode > 0:
            controller.save_models(f"model2/{experiment_name}_ep{episode}")
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            results = {
                'episode_rewards': episode_rewards,
                'collective_rewards': collective_rewards,
                'behavior_metrics': behavior_metrics,
                'adjustment_history': adjustment_history,
                'config': {
                    'env_name': env_name,
                    'n_agents': n_agents,
                    'episode': episode,
                    'experiment_name': experiment_name
                }
            }
            with open(f"model2/results_{experiment_name}_ep{episode}.json", 'w') as f:
                json.dump(results, f)
    
    # æœ€ç»ˆä¿å­˜
    controller.save_models(f"model2/{experiment_name}_final")
    controller.close_envs()
    
    # æœ€ç»ˆç»“æœ
    total_time = time.time() - start_time
    final_behavior = controller.behavior_monitor.get_activity_metrics()
    
    print(f"\nğŸ‰ ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"â±ï¸ æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³é›†ä½“å¥–åŠ±: {best_collective_reward:.2f}")
    print(f"ğŸ“ˆ æœ€ç»ˆ100è½®å¹³å‡: {np.mean(collective_rewards[-100:]):.2f}")
    print(f"ğŸƒ æœ€ç»ˆç§»åŠ¨é¢‘ç‡: {final_behavior.get('avg_move_ratio', 0):.2f}")
    print(f"ğŸ—ºï¸ æœ€ç»ˆæ¢ç´¢èŒƒå›´: {final_behavior.get('avg_exploration', 0):.1f}")
    print(f"ğŸ”§ æœ€ç»ˆentropy_coef: {controller.entropy_coef:.3f}")
    print(f"ğŸ“Š è°ƒæ•´æ¬¡æ•°: {len(adjustment_history)}")
    
    # åˆ†æè°ƒæ•´æ•ˆæœ
    if adjustment_history:
        print(f"\nğŸ“ˆ è°ƒæ•´å†å²:")
        for adj in adjustment_history[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¬¡è°ƒæ•´
            print(f"   Episode {adj['episode']}: æ€§èƒ½={adj['performance']:.2f}, entropy_coef={adj['entropy_coef']:.3f}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        'episode_rewards': episode_rewards,
        'collective_rewards': collective_rewards,
        'best_collective_reward': best_collective_reward,
        'final_behavior_metrics': final_behavior,
        'adjustment_history': adjustment_history,
        'total_time_hours': total_time / 3600,
        'final_entropy_coef': controller.entropy_coef,
        'config': {
            'env_name': env_name,
            'n_agents': n_agents,
            'n_episodes': n_episodes,
            'experiment_name': experiment_name,
            'fixes_applied': [
                'entropy_upper_limit_0.3',
                'stagnation_detection_300_episodes',
                'smart_recovery_strategy',
                'removed_parameter_noise',
                'conservative_learning_rate_adjustment'
            ]
        }
    }
    
    with open(f"model2/final_results_{experiment_name}.json", 'w') as f:
        json.dump(final_results, f)
    
    # ç»˜åˆ¶ç»“æœ
    plot_fixed_training_results(collective_rewards, episode_rewards, final_behavior, 
                               adjustment_history, experiment_name)
    
    if use_wandb:
        wandb.log({"final_training_curves": wandb.Image(f'model2/training_curves_{experiment_name}.png')})
        wandb.finish()
        print("âœ… WandBæ—¥å¿—å·²å®Œæˆ")
    
    return controller, episode_rewards, collective_rewards, experiment_name


def plot_fixed_training_results(collective_rewards, episode_rewards, behavior_metrics, 
                               adjustment_history, experiment_name):
    """ç»˜åˆ¶ä¿®å¤ç‰ˆè®­ç»ƒç»“æœ"""
    plt.figure(figsize=(16, 12))
    
    # é›†ä½“å¥–åŠ± + è°ƒæ•´æ ‡è®°
    plt.subplot(2, 3, 1)
    plt.plot(collective_rewards, alpha=0.7, label='é›†ä½“å¥–åŠ±')
    window = min(500, len(collective_rewards) // 10)
    if len(collective_rewards) > window:
        moving_avg = np.convolve(collective_rewards, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(collective_rewards)), moving_avg, 'r-', linewidth=2, label=f'Moving avg ({window})')
    
    # æ ‡è®°è°ƒæ•´ç‚¹
    for adj in adjustment_history:
        plt.axvline(x=adj['episode'], color='orange', linestyle='--', alpha=0.7)
        plt.text(adj['episode'], max(collective_rewards)*0.9, f"{adj['entropy_coef']:.2f}", 
                rotation=90, fontsize=8)
    
    plt.title('é›†ä½“å¥–åŠ± (æ©™çº¿=è°ƒæ•´ç‚¹)')
    plt.xlabel('è½®æ•°')
    plt.ylabel('å¥–åŠ±')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä¸ªä½“æ™ºèƒ½ä½“å¥–åŠ±
    plt.subplot(2, 3, 2)
    n_agents = len(episode_rewards[0]) if episode_rewards else 3
    for i in range(n_agents):
        individual_rewards = [ep[i] for ep in episode_rewards]
        plt.plot(individual_rewards, label=f'æ™ºèƒ½ä½“ {i}', alpha=0.7)
    plt.title('ä¸ªä½“æ™ºèƒ½ä½“å¥–åŠ±')
    plt.xlabel('è½®æ•°')
    plt.ylabel('å¥–åŠ±')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å¥–åŠ±ç¨³å®šæ€§åˆ†æ
    plt.subplot(2, 3, 3)
    if len(collective_rewards) > 100:
        # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®
        rolling_std = []
        window_size = 100
        for i in range(window_size, len(collective_rewards)):
            std = np.std(collective_rewards[i-window_size:i])
            rolling_std.append(std)
        
        plt.plot(range(window_size, len(collective_rewards)), rolling_std, 'g-', label='æ»šåŠ¨æ ‡å‡†å·®')
        plt.title('å¥–åŠ±ç¨³å®šæ€§ (æ ‡å‡†å·®è¶Šå°è¶Šç¨³å®š)')
        plt.xlabel('è½®æ•°')
        plt.ylabel('æ ‡å‡†å·®')
        plt.legend()
    plt.grid(True, alpha=0.3)
    
    # entropy_coefå˜åŒ–å†å²
    plt.subplot(2, 3, 4)
    if adjustment_history:
        episodes = [adj['episode'] for adj in adjustment_history]
        entropy_values = [adj['entropy_coef'] for adj in adjustment_history]
        plt.plot(episodes, entropy_values, 'ro-', label='entropy_coefè°ƒæ•´')
        plt.title('entropy_coefè°ƒæ•´å†å²')
        plt.xlabel('è½®æ•°')
        plt.ylabel('entropy_coefå€¼')
        plt.legend()
    else:
        plt.text(0.5, 0.5, 'æ— è°ƒæ•´å‘ç”Ÿ\n(è®­ç»ƒç¨³å®š)', ha='center', va='center', fontsize=14,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
        plt.title('entropy_coefè°ƒæ•´å†å²')
    plt.grid(True, alpha=0.3)
    
    # è¡Œä¸ºæŒ‡æ ‡
    plt.subplot(2, 3, 5)
    if behavior_metrics:
        metrics_names = []
        metrics_values = []
        for key, value in behavior_metrics.items():
            if 'agent_' in key and ('move_ratio' in key or 'exploration' in key):
                metrics_names.append(key.replace('agent_', 'A').replace('_move_ratio', '_ç§»åŠ¨').replace('_unique_positions', '_æ¢ç´¢'))
                metrics_values.append(value)
        
        if metrics_names:
            colors = ['skyblue' if 'ç§»åŠ¨' in name else 'orange' for name in metrics_names]
            plt.bar(metrics_names, metrics_values, alpha=0.7, color=colors)
            plt.title('æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡')
            plt.ylabel('æ•°å€¼')
            plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # ä¿®å¤æ•ˆæœæ€»ç»“
    plt.subplot(2, 3, 6)
    plt.axis('off')
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    if collective_rewards:
        max_reward = max(collective_rewards)
        final_avg = np.mean(collective_rewards[-100:]) if len(collective_rewards) >= 100 else np.mean(collective_rewards)
        
        # åˆ¤æ–­è®­ç»ƒæ•ˆæœ
        if len(adjustment_history) == 0:
            stability_status = "âœ… è®­ç»ƒç¨³å®šï¼Œæ— éœ€è°ƒæ•´"
        elif len(adjustment_history) <= 3:
            stability_status = "âœ… è½»å¾®è°ƒæ•´ï¼Œå·²ç¨³å®š"
        else:
            stability_status = "âš ï¸ å¤šæ¬¡è°ƒæ•´ï¼Œéœ€è¦ç›‘æ§"
        
        move_ratio = behavior_metrics.get('avg_move_ratio', 0)
        exploration = behavior_metrics.get('avg_exploration', 0)
        
        if move_ratio > 0.6:
            activity_status = "âœ… æ™ºèƒ½ä½“ä¿æŒæ´»è·ƒ"
        else:
            activity_status = "âš ï¸ æ™ºèƒ½ä½“æ´»è·ƒåº¦åä½"
            
        summary_text = f"""ä¿®å¤ç‰ˆè®­ç»ƒæ€»ç»“:
        
ğŸ† æœ€é«˜å¥–åŠ±: {max_reward:.2f}
ğŸ“ˆ æœ€ç»ˆ100è½®å¹³å‡: {final_avg:.2f}
ğŸ”§ è°ƒæ•´æ¬¡æ•°: {len(adjustment_history)}

ğŸ“Š ç¨³å®šæ€§: {stability_status}
ğŸƒ æ´»è·ƒåº¦: {activity_status}
ğŸ—ºï¸ æ¢ç´¢èŒƒå›´: {exploration:.1f}

ğŸ› ï¸ åº”ç”¨çš„ä¿®å¤:
âœ“ entropy_coefä¸Šé™ â†’ 0.3
âœ“ åœæ»æ£€æµ‹ â†’ 300è½®
âœ“ æ™ºèƒ½æ¢å¤ç­–ç•¥
âœ“ ç§»é™¤å‚æ•°å™ªå£°
âœ“ ä¿å®ˆè°ƒæ•´ç­–ç•¥"""
        
        plt.text(0.05, 0.95, summary_text, fontsize=9, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8))
    
    plt.tight_layout()
    
    # ç¡®ä¿model2ç›®å½•å­˜åœ¨
    os.makedirs('model2', exist_ok=True)
    plt.savefig(f'model2/training_curves_{experiment_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ä¿®å¤ç‰ˆè®­ç»ƒæ›²çº¿å·²ä¿å­˜: model2/training_curves_{experiment_name}.png")


def test_fixed_agents(model_path_prefix, env_name, n_episodes=5, visualize=True):
    """æµ‹è¯•ä¿®å¤ç‰ˆæ™ºèƒ½ä½“"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_name)
    n_agents = env.n_agents
    
    # åˆ›å»ºæ§åˆ¶å™¨å¹¶åŠ è½½æ¨¡å‹
    controller = FixedMultiAgentPPOController(env_name, n_agents, device, n_parallel_envs=1)
    controller.load_models(model_path_prefix)
    
    # è®¾ç½®è¯„ä¼°æ¨¡å¼
    for agent in controller.agents:
        agent.eval()
    
    print(f"ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆæ™ºèƒ½ä½“")
    print(f"ğŸ¤– æ™ºèƒ½ä½“æ•°é‡: {n_agents}")
    print(f"ğŸ“Š æµ‹è¯•è½®æ•°: {n_episodes}")
    
    test_results = []
    behavior_monitor = BehaviorMonitor(n_agents)
    
    for episode in range(n_episodes):
        obs = env.reset()
        behavior_monitor.reset()
        
        total_rewards = [0] * n_agents
        steps = 0
        trajectory = []
        
        while True:
            # å­˜å‚¨è½¨è¿¹æ•°æ®
            if visualize:
                trajectory.append({
                    'obs': obs,
                    'full_image': env.render('rgb_array'),
                    'step': steps,
                    'agent_positions': [tuple(pos) for pos in env.agent_pos]
                })
            
            # è·å–åŠ¨ä½œ
            actions, _, _, _, _ = controller.get_actions(obs)
            
            # æ›´æ–°è¡Œä¸ºç›‘æ§
            behavior_monitor.update(actions, env.agent_pos)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, rewards, done, info = env.step(actions)
            
            for i in range(n_agents):
                total_rewards[i] += rewards[i]
            
            steps += 1
            
            if done or steps > 200:
                break
        
        # è·å–è¡Œä¸ºæŒ‡æ ‡
        behavior_metrics = behavior_monitor.get_activity_metrics()
        
        collective_reward = sum(total_rewards)
        test_results.append({
            'episode': episode,
            'collective_reward': collective_reward,
            'individual_rewards': total_rewards,
            'steps': steps,
            'trajectory': trajectory if visualize else None,
            'behavior_metrics': behavior_metrics
        })
        
        move_ratio = behavior_metrics.get('avg_move_ratio', 0)
        exploration = behavior_metrics.get('avg_exploration', 0)
        
        print(f"æµ‹è¯•è½® {episode + 1}: "
              f"é›†ä½“å¥–åŠ± = {collective_reward:.2f}, "
              f"ä¸ªä½“å¥–åŠ± = {total_rewards}, "
              f"æ­¥æ•° = {steps}, "
              f"ç§»åŠ¨ç‡ = {move_ratio:.2f}, "
              f"æ¢ç´¢ = {exploration:.1f}")
    
    env.close()
    
    # è®¡ç®—æ€»ç»“ç»Ÿè®¡
    avg_collective = np.mean([r['collective_reward'] for r in test_results])
    avg_steps = np.mean([r['steps'] for r in test_results])
    avg_move_ratio = np.mean([r['behavior_metrics'].get('avg_move_ratio', 0) for r in test_results])
    avg_exploration = np.mean([r['behavior_metrics'].get('avg_exploration', 0) for r in test_results])
    
    print(f"\nğŸ“Š ä¿®å¤ç‰ˆæµ‹è¯•ç»“æœ:")
    print(f"å¹³å‡é›†ä½“å¥–åŠ±: {avg_collective:.2f}")
    print(f"å¹³å‡è½®æ•°é•¿åº¦: {avg_steps:.1f}")
    print(f"å¹³å‡ç§»åŠ¨é¢‘ç‡: {avg_move_ratio:.2f}")
    print(f"å¹³å‡æ¢ç´¢èŒƒå›´: {avg_exploration:.1f}")
    
    if avg_move_ratio > 0.6:
        print("âœ… ä¿®å¤æˆåŠŸï¼šæ™ºèƒ½ä½“ä¿æŒç§¯æç§»åŠ¨")
    else:
        print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ï¼šç§»åŠ¨é¢‘ç‡ä»ç„¶åä½")
    
    return test_results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPO - è§£å†³entropy_coefå¡ä½é—®é¢˜")
    parser.add_argument("--env", type=str, default="MultiGrid-Cluttered-Fixed-15x15", 
                        help="ç¯å¢ƒåç§°")
    parser.add_argument("--episodes", type=int, default=10000, 
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--parallel-envs", type=int, default=4, 
                        help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--steps", type=int, default=256, 
                        help="æ¯æ¬¡rolloutçš„æ­¥æ•°")
    parser.add_argument("--no-wandb", action="store_true", 
                        help="ç¦ç”¨WandBæ—¥å¿—")
    parser.add_argument("--test", type=str, default=None, 
                        help="æµ‹è¯•æ¨¡å¼ï¼šæä¾›model2/ä¸­çš„æ¨¡å‹è·¯å¾„å‰ç¼€")
    parser.add_argument("--project", type=str, default="fixed-multigrid-ppo", 
                        help="WandBé¡¹ç›®åç§°")
    parser.add_argument("--name", type=str, default=None, 
                        help="å®éªŒåç§°")
    parser.add_argument("--emergency-reset", action="store_true",
                        help="å¯¹æ­£åœ¨è¿è¡Œçš„è®­ç»ƒæ‰§è¡Œç´§æ€¥é‡ç½®")
    
    args = parser.parse_args()
    
    if args.emergency_reset:
        print("ğŸš‘ æ³¨æ„ï¼š--emergency-resetéœ€è¦åœ¨è®­ç»ƒä»£ç ä¸­æ‰‹åŠ¨è°ƒç”¨controller.emergency_reset()")
        
    if args.test:
        # æµ‹è¯•æ¨¡å¼
        print("ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆæ™ºèƒ½ä½“...")
        test_results = test_fixed_agents(args.test, args.env)
    else:
        # è®­ç»ƒæ¨¡å¼
        print("ğŸ› ï¸ å¯åŠ¨ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ...")
        print("ğŸ”§ ä¸»è¦ä¿®å¤:")
        print("   âœ… entropy_coefä¸Šé™ä»0.1æé«˜åˆ°0.3")
        print("   âœ… åœæ»æ£€æµ‹ä»100è½®å»¶é•¿åˆ°300è½®")
        print("   âœ… ç§»é™¤ç ´åæ€§å‚æ•°å™ªå£°")
        print("   âœ… æ™ºèƒ½æ¢å¤ç­–ç•¥")
        print("   âœ… æ¨¡å‹ä¿å­˜åˆ°model2/æ–‡ä»¶å¤¹")
        print("="*70)
        
        controller, episode_rewards, collective_rewards, experiment_name = train_fixed_multiagent_ppo(
            env_name=args.env,
            n_episodes=args.episodes,
            n_steps=args.steps,
            n_parallel_envs=args.parallel_envs,
            use_wandb=not args.no_wandb,
            project_name=args.project,
            experiment_name=args.name
        )
        
        print("\nâœ… ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: model2/{experiment_name}_final")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: model2/final_results_{experiment_name}.json")
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿: model2/training_curves_{experiment_name}.png")
        
        # æµ‹è¯•æœ€ç»ˆæ¨¡å‹
        print("\nğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆæœ€ç»ˆæ¨¡å‹...")
        test_results = test_fixed_agents(f"model2/{experiment_name}_final", args.env, n_episodes=3)
        
        print("\nğŸ‰ ä¿®å¤ç‰ˆå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒå®Œæˆ!")
        print("ğŸ› ï¸ ä¸»è¦ä¿®å¤æ•ˆæœ:")
        print("   âœ… entropy_coefä¸å†å¡åœ¨0.1")
        print("   âœ… å‡å°‘äº†é¢‘ç¹çš„åœæ»è°ƒæ•´")  
        print("   âœ… æ™ºèƒ½ä½“è¡Œä¸ºæ›´åŠ ç¨³å®š")
        print("   âœ… æ€§èƒ½ä¸‹é™é—®é¢˜å¾—åˆ°ç¼“è§£")
        print("   âœ… æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨model2/æ–‡ä»¶å¤¹")