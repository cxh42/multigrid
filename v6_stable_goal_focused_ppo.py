"""
v4ç¨³å®šç‰ˆå¤šæ™ºèƒ½ä½“PPO - ä¿®å¤è®­ç»ƒå´©æºƒé—®é¢˜

ä¿®å¤ï¼š
1. é™ä½å­¦ä¹ ç‡ï¼š3e-4 â†’ 1e-4
2. ä¼˜åŒ–å¥–åŠ±è®¾è®¡ï¼Œé¿å…è´Ÿå¥–åŠ±ç´¯ç§¯
3. æ·»åŠ æ¢¯åº¦ç›‘æ§å’Œæ—©åœæœºåˆ¶
4. æ›´ä¿å®ˆçš„å¥–åŠ±å¡‘å½¢
5. æ·»åŠ è°ƒè¯•è¾“å‡º
"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions.categorical import Categorical
import argparse
import matplotlib.pyplot as plt
import os
import time
import json
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from envs import gym_multigrid
from envs.gym_multigrid import multigrid_envs

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False


class StableRewardShaper:
    """ç¨³å®šçš„å¥–åŠ±å¡‘å½¢å™¨ - é¿å…è´Ÿå¥–åŠ±ç´¯ç§¯"""
    
    def __init__(self, n_agents):
        self.n_agents = n_agents
        self.reset()
        
    def reset(self):
        self.prev_distances = [None] * self.n_agents
        self.prev_positions = [None] * self.n_agents
        self.stationary_count = [0] * self.n_agents
        
    def get_goal_position(self, env):
        """è·å–ç›®æ ‡ä½ç½®"""
        try:
            for i in range(env.width):
                for j in range(env.height):
                    cell = env.grid.get(i, j)
                    if cell and cell.type == 'goal':
                        return np.array([i, j])
            return np.array([13, 13])
        except:
            return np.array([13, 13])
    
    def shape_rewards(self, env, agent_positions, original_rewards, actions):
        """ç¨³å®šçš„å¥–åŠ±å¡‘å½¢ - é¿å…å¤§è´Ÿå€¼"""
        shaped_rewards = list(original_rewards)
        goal_pos = self.get_goal_position(env)
        
        for i in range(self.n_agents):
            pos = np.array(agent_positions[i])
            action = actions[i]
            
            # 1. è§¦ç¢°ç›®æ ‡ - å·¨å¤§æ­£å¥–åŠ±
            if original_rewards[i] > 0:
                shaped_rewards[i] = 10.0  # é™ä½åˆ°10ï¼Œæ›´ç¨³å®š
                print(f"ğŸ¯ æ™ºèƒ½ä½“{i}è§¦ç¢°ç›®æ ‡ï¼å¥–åŠ±+10")
                continue
            
            # é‡ç½®shaped_rewardsä¸º0ï¼Œé¿å…ç´¯ç§¯åŸå§‹è´Ÿå¥–åŠ±
            shaped_rewards[i] = 0.0
            
            # 2. è·ç¦»å¥–åŠ± - æ›´ä¿å®ˆ
            current_dist = np.linalg.norm(pos - goal_pos)
            if self.prev_distances[i] is not None:
                dist_change = self.prev_distances[i] - current_dist
                shaped_rewards[i] += dist_change * 0.2  # é™ä½åˆ°0.2
            self.prev_distances[i] = current_dist
            
            # 3. é™æ­¢æƒ©ç½š - æ›´æ¸©å’Œä¸”æœ‰ä¸Šé™
            if self.prev_positions[i] is not None:
                if np.array_equal(pos, self.prev_positions[i]):
                    self.stationary_count[i] += 1
                    # åªæœ‰è¿ç»­é™æ­¢æ‰æƒ©ç½šï¼Œä¸”æœ‰ä¸Šé™
                    if self.stationary_count[i] > 3:
                        shaped_rewards[i] -= min(0.05, self.stationary_count[i] * 0.01)
                else:
                    self.stationary_count[i] = 0
                    shaped_rewards[i] += 0.01  # ç§»åŠ¨å¥–åŠ±
                    
            # 4. ç§»åŠ¨å¥–åŠ± - é¼“åŠ±è¡ŒåŠ¨
            if action == 2:  # forward
                shaped_rewards[i] += 0.02
            elif action in [0, 1]:  # turn
                shaped_rewards[i] += 0.005
                
            # 5. å»æ‰æ—¶é—´æƒ©ç½šï¼Œé¿å…ç´¯ç§¯è´Ÿå€¼
            
            # 6. ç¡®ä¿å¥–åŠ±ä¸ä¼šå¤ªè´Ÿ
            shaped_rewards[i] = max(shaped_rewards[i], -0.2)
            
            self.prev_positions[i] = pos.copy()
            
        return shaped_rewards


class SimplePPOAgent(nn.Module):
    """ç®€åŒ–çš„PPOç½‘ç»œ"""
    
    def __init__(self, n_actions=7):
        super().__init__()
        
        # å›¾åƒå¤„ç†
        self.image_conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1), 
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        # æ–¹å‘åµŒå…¥
        self.direction_embed = nn.Embedding(4, 8)
        
        # å…±äº«ç½‘ç»œ
        self.shared = nn.Sequential(
            nn.Linear(64 + 8, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
        )
        
        # è¾“å‡ºå¤´
        self.actor = nn.Linear(64, n_actions)
        self.critic = nn.Linear(64, 1)
        
        # åˆå§‹åŒ–æƒé‡ - æ›´ä¿å®ˆ
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, 0.5)
            module.bias.data.fill_(0.0)
        elif isinstance(module, nn.Conv2d):
            torch.nn.init.orthogonal_(module.weight, 1.0)
            
    def forward(self, obs):
        image = obs['image']
        direction = obs['direction']
        
        # å¤„ç†å›¾åƒ
        if len(image.shape) == 4:
            image = image.permute(0, 3, 1, 2)
        elif len(image.shape) == 3:
            image = image.permute(2, 0, 1).unsqueeze(0)
        
        # å¤„ç†æ–¹å‘
        if len(direction.shape) > 1:
            direction = direction.squeeze(-1)
        if len(direction.shape) == 0:
            direction = direction.unsqueeze(0)
        
        # ç‰¹å¾æå–
        image_features = self.image_conv(image.float())
        direction_features = self.direction_embed(direction.long())
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        batch_size = image_features.shape[0]
        if direction_features.shape[0] != batch_size:
            direction_features = direction_features.repeat(batch_size, 1)
        
        # åˆå¹¶ç‰¹å¾
        combined = torch.cat([image_features, direction_features], dim=-1)
        shared_features = self.shared(combined)
        
        # è¾“å‡º
        action_logits = self.actor(shared_features)
        value = self.critic(shared_features)
        
        return action_logits, value.squeeze(-1)
    
    def get_action_and_value(self, obs, action=None):
        action_logits, value = self.forward(obs)
        dist = Categorical(logits=action_logits)
        
        if action is None:
            action = dist.sample()
        
        return action, dist.log_prob(action), dist.entropy(), value


class StablePPOController:
    """ç¨³å®šç‰ˆPPOæ§åˆ¶å™¨"""
    
    def __init__(self, env_name, n_agents, device, lr=1e-4, n_parallel_envs=4):  # é™ä½å­¦ä¹ ç‡
        self.env_name = env_name
        self.n_agents = n_agents
        self.device = device
        self.n_parallel_envs = n_parallel_envs
        
        # åˆ›å»ºæ™ºèƒ½ä½“å’Œä¼˜åŒ–å™¨
        self.agents = []
        self.optimizers = []
        
        for i in range(n_agents):
            agent = SimplePPOAgent(n_actions=7).to(device)
            optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)
            
            self.agents.append(agent)
            self.optimizers.append(optimizer)
        
        # åˆ›å»ºç¯å¢ƒ
        self.envs = [gym.make(env_name) for _ in range(n_parallel_envs)]
        
        # ç¨³å®šçš„å¥–åŠ±å¡‘å½¢å™¨
        self.reward_shapers = [StableRewardShaper(n_agents) for _ in range(n_parallel_envs)]
        
        # è®­ç»ƒç›‘æ§
        self.entropy_coef = 0.01
        self.performance_history = []
        self.gradient_norms = []
        
        self.reset_buffers()
        
        print(f"âœ… åˆ›å»ºäº† {n_agents} ä¸ªç¨³å®šPPOæ™ºèƒ½ä½“")
        print(f"âœ… å­¦ä¹ ç‡: {lr} (é™ä½ä»¥æé«˜ç¨³å®šæ€§)")
        print(f"âœ… ä½¿ç”¨ {n_parallel_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")
        print(f"âœ… è®¾å¤‡: {device}")
    
    def reset_buffers(self):
        """é‡ç½®ç»éªŒç¼“å†²åŒº"""
        self.buffers = []
        for i in range(self.n_agents):
            self.buffers.append({
                'observations': [],
                'actions': [],
                'log_probs': [],
                'values': [],
                'rewards': [],
                'dones': []
            })
    
    def get_parallel_actions(self, obs_list):
        """è·å–å¹¶è¡Œç¯å¢ƒçš„åŠ¨ä½œ"""
        all_actions = []
        all_log_probs = []
        all_values = []
        all_individual_obs = []
        
        for env_idx, obs in enumerate(obs_list):
            env_actions = []
            env_log_probs = []
            env_values = []
            env_individual_obs = []
            
            for i in range(self.n_agents):
                agent_obs = {
                    'image': torch.FloatTensor(obs['image'][i]).to(self.device),
                    'direction': torch.LongTensor([obs['direction'][i]]).to(self.device)
                }
                
                with torch.no_grad():
                    action, log_prob, entropy, value = self.agents[i].get_action_and_value(agent_obs)
                
                env_actions.append(action.item())
                env_log_probs.append(log_prob.item())
                env_values.append(value.item())
                env_individual_obs.append(agent_obs)
            
            all_actions.append(env_actions)
            all_log_probs.append(env_log_probs)
            all_values.append(env_values)
            all_individual_obs.append(env_individual_obs)
        
        return all_actions, all_log_probs, all_values, all_individual_obs
    
    def collect_rollout(self, n_steps=128):
        """æ”¶é›†ç»éªŒ"""
        self.reset_buffers()
        
        # é‡ç½®ç¯å¢ƒ
        obs_list = []
        for i, env in enumerate(self.envs):
            obs = env.reset()
            obs_list.append(obs)
            self.reward_shapers[i].reset()
        
        total_goal_touches = 0
        step_rewards = []
        
        for step in range(n_steps):
            # è·å–åŠ¨ä½œ
            all_actions, all_log_probs, all_values, all_individual_obs = self.get_parallel_actions(obs_list)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs_list = []
            rewards_list = []
            dones_list = []
            
            for env_idx, env in enumerate(self.envs):
                next_obs, rewards, done, info = env.step(all_actions[env_idx])
                
                # ç¨³å®šçš„å¥–åŠ±å¡‘å½¢
                agent_positions = [env.agent_pos[i] for i in range(self.n_agents)]
                shaped_rewards = self.reward_shapers[env_idx].shape_rewards(
                    env, agent_positions, rewards, all_actions[env_idx])
                
                # ç»Ÿè®¡ç›®æ ‡è§¦ç¢°
                for r in shaped_rewards:
                    if r >= 9.5:  # è§¦ç¢°ç›®æ ‡çš„å¥–åŠ±
                        total_goal_touches += 1
                
                next_obs_list.append(next_obs)
                rewards_list.append(shaped_rewards)
                dones_list.append(done)
                
                step_rewards.extend(shaped_rewards)
            
            # å­˜å‚¨ç»éªŒ
            for env_idx in range(self.n_parallel_envs):
                for agent_idx in range(self.n_agents):
                    self.buffers[agent_idx]['observations'].append(all_individual_obs[env_idx][agent_idx])
                    self.buffers[agent_idx]['actions'].append(all_actions[env_idx][agent_idx])
                    self.buffers[agent_idx]['log_probs'].append(all_log_probs[env_idx][agent_idx])
                    self.buffers[agent_idx]['values'].append(all_values[env_idx][agent_idx])
                    self.buffers[agent_idx]['rewards'].append(rewards_list[env_idx][agent_idx])
                    self.buffers[agent_idx]['dones'].append(dones_list[env_idx])
            
            # é‡ç½®å®Œæˆçš„ç¯å¢ƒ
            for env_idx, done in enumerate(dones_list):
                if done:
                    obs_list[env_idx] = self.envs[env_idx].reset()
                    self.reward_shapers[env_idx].reset()
                else:
                    obs_list[env_idx] = next_obs_list[env_idx]
        
        # è°ƒè¯•è¾“å‡º
        avg_step_reward = np.mean(step_rewards) if step_rewards else 0
        print(f"Rolloutå®Œæˆ: å¹³å‡æ­¥å¥–åŠ±={avg_step_reward:.3f}, ç›®æ ‡è§¦ç¢°={total_goal_touches}")
        
        return self.buffers, total_goal_touches
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—GAE"""
        advantages = []
        returns = []
        
        advantage = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            next_non_terminal = 1 - dones[t]
            delta = rewards[t] + gamma * next_value * next_non_terminal - values[t]
            advantage = delta + gamma * gae_lambda * next_non_terminal * advantage
            
            advantages.insert(0, advantage)
            returns.insert(0, advantage + values[t])
        
        return advantages, returns
    
    def update_agent(self, agent_id, n_epochs=4, batch_size=64, clip_coef=0.2):
        """æ›´æ–°å•ä¸ªæ™ºèƒ½ä½“ - æ·»åŠ æ¢¯åº¦ç›‘æ§"""
        buffer = self.buffers[agent_id]
        agent = self.agents[agent_id]
        optimizer = self.optimizers[agent_id]
        
        if len(buffer['rewards']) == 0:
            return {}
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages, returns = self.compute_gae(
            buffer['rewards'], buffer['values'], buffer['dones'])
        
        # è½¬æ¢ä¸ºå¼ é‡
        observations = buffer['observations']
        actions = torch.LongTensor(buffer['actions']).to(self.device)
        old_log_probs = torch.FloatTensor(buffer['log_probs']).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
        
        # PPOæ›´æ–°
        total_loss = 0
        total_grad_norm = 0
        n_updates = 0
        
        for epoch in range(n_epochs):
            indices = torch.randperm(len(observations))
            
            for start in range(0, len(observations), batch_size):
                end = min(start + batch_size, len(observations))
                batch_indices = indices[start:end]
                
                # å‡†å¤‡æ‰¹æ¬¡
                batch_images = torch.stack([observations[i]['image'] for i in batch_indices])
                batch_directions = torch.stack([observations[i]['direction'] for i in batch_indices])
                batch_directions = batch_directions.squeeze(-1)
                
                batch_obs = {
                    'image': batch_images,
                    'direction': batch_directions
                }
                
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages_tensor[batch_indices]
                batch_returns = returns_tensor[batch_indices]
                
                # å‰å‘ä¼ æ’­
                _, new_log_probs, entropy, new_values = agent.get_action_and_value(batch_obs, batch_actions)
                
                # è®¡ç®—æŸå¤±
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                value_loss = 0.5 * (new_values - batch_returns).pow(2).mean()
                entropy_loss = -self.entropy_coef * entropy.mean()
                
                total_loss_batch = policy_loss + value_loss + entropy_loss
                
                # æ›´æ–°
                optimizer.zero_grad()
                total_loss_batch.backward()
                
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                grad_norm = torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)
                optimizer.step()
                
                total_loss += total_loss_batch.item()
                total_grad_norm += grad_norm.item()
                n_updates += 1
        
        avg_grad_norm = total_grad_norm / max(1, n_updates)
        self.gradient_norms.append(avg_grad_norm)
        
        # æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
        if avg_grad_norm > 10.0:
            print(f"âš ï¸ æ™ºèƒ½ä½“{agent_id}æ¢¯åº¦èŒƒæ•°å¼‚å¸¸: {avg_grad_norm:.2f}")
        
        return {
            'loss': total_loss / max(1, n_updates),
            'grad_norm': avg_grad_norm
        }
    
    def update_all_agents(self):
        """æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“"""
        losses = []
        grad_norms = []
        
        for i in range(self.n_agents):
            metrics = self.update_agent(i)
            if 'loss' in metrics:
                losses.append(metrics['loss'])
                grad_norms.append(metrics['grad_norm'])
        
        return {
            'avg_loss': np.mean(losses) if losses else 0,
            'avg_grad_norm': np.mean(grad_norms) if grad_norms else 0
        }
    
    def check_training_health(self, current_reward, episode):
        """æ£€æŸ¥è®­ç»ƒå¥åº·çŠ¶å†µ"""
        self.performance_history.append(current_reward)
        
        if len(self.performance_history) < 100:
            return True
        
        # æ£€æŸ¥æ˜¯å¦å‡ºç°ä¸¥é‡ä¸‹é™
        recent_avg = np.mean(self.performance_history[-50:])
        older_avg = np.mean(self.performance_history[-100:-50])
        
        if recent_avg < older_avg - 100:  # ä¸‹é™è¶…è¿‡100
            print(f"ğŸš¨ è­¦å‘Šï¼šè®­ç»ƒå¯èƒ½å‡ºç°é—®é¢˜ï¼")
            print(f"   æœ€è¿‘50è½®å¹³å‡: {recent_avg:.2f}")
            print(f"   ä¹‹å‰50è½®å¹³å‡: {older_avg:.2f}")
            print(f"   ä¸‹é™å¹…åº¦: {older_avg - recent_avg:.2f}")
            
            # æ£€æŸ¥æ¢¯åº¦
            if len(self.gradient_norms) > 0:
                recent_grad = np.mean(self.gradient_norms[-10:])
                print(f"   æœ€è¿‘æ¢¯åº¦èŒƒæ•°: {recent_grad:.4f}")
            
            return False
        
        return True
    
    def save_models(self, path_prefix):
        """ä¿å­˜æ¨¡å‹"""
        if "models6" not in path_prefix:
            path_prefix = path_prefix.replace("models/", "models6/").replace("model2/", "models6/")
        
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else "models6", exist_ok=True)
        for i in range(self.n_agents):
            path = f"{path_prefix}_agent_{i}.pth"
            torch.save(self.agents[i].state_dict(), path)
        print(f"âœ… å·²ä¿å­˜æ¨¡å‹åˆ°: {path_prefix}_agent_*.pth")
    
    def load_models(self, path_prefix):
        """åŠ è½½æ¨¡å‹"""
        if "models6" not in path_prefix:
            path_prefix = path_prefix.replace("models/", "models6/").replace("model2/", "models6/")
            
        for i in range(self.n_agents):
            path = f"{path_prefix}_agent_{i}.pth"
            if os.path.exists(path):
                self.agents[i].load_state_dict(torch.load(path, map_location=self.device))
            else:
                print(f"Warning: æ¨¡å‹æ–‡ä»¶ {path} ä¸å­˜åœ¨")
        print(f"âœ… å·²åŠ è½½æ¨¡å‹: {path_prefix}_agent_*.pth")
    
    def close_envs(self):
        """å…³é—­ç¯å¢ƒ"""
        for env in self.envs:
            env.close()


def train_stable_ppo(
    env_name="MultiGrid-Cluttered-Fixed-15x15",
    n_episodes=50000,  # ç¬¦åˆæ–‡æ¡£è¦æ±‚
    n_steps=128,
    n_parallel_envs=4,
    use_wandb=True,
    project_name="stable-goal-focused-ppo",
    experiment_name=None
):
    """ç¨³å®šç‰ˆPPOè®­ç»ƒ"""
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("ğŸ›¡ï¸ å¯åŠ¨ç¨³å®šç‰ˆç›®æ ‡å¯¼å‘å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ")
    print("="*60)
    print("ğŸ”§ ç¨³å®šæ€§æ”¹è¿›:")
    print("   âœ… å­¦ä¹ ç‡é™ä½: 3e-4 â†’ 1e-4")
    print("   âœ… å¥–åŠ±è®¾è®¡ä¼˜åŒ–ï¼Œé¿å…è´Ÿå€¼ç´¯ç§¯")
    print("   âœ… æ·»åŠ æ¢¯åº¦ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹")
    print("   âœ… æ›´ä¿å®ˆçš„å¥–åŠ±å¡‘å½¢")
    print("   âœ… æ—©åœæœºåˆ¶é˜²æ­¢è®­ç»ƒå´©æºƒ")
    print("="*60)
    
    # è·å–ç¯å¢ƒä¿¡æ¯
    temp_env = gym.make(env_name)
    n_agents = temp_env.n_agents
    temp_env.close()
    
    print(f"ğŸ¯ ç¯å¢ƒ: {env_name}")
    print(f"ğŸ¤– æ™ºèƒ½ä½“æ•°é‡: {n_agents}")
    print(f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {n_episodes}")
    print(f"ğŸ”„ å¹¶è¡Œç¯å¢ƒ: {n_parallel_envs}")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # å®éªŒåç§°
    if experiment_name is None:
        experiment_name = f"stable_{env_name}_{n_agents}agents_{int(time.time())}"
    
    # WandBåˆå§‹åŒ–
    if use_wandb and WANDB_AVAILABLE:
        try:
            wandb.init(
                project=project_name,
                name=experiment_name,
                config={
                    "env_name": env_name,
                    "n_agents": n_agents,
                    "n_episodes": n_episodes,
                    "n_steps": n_steps,
                    "n_parallel_envs": n_parallel_envs,
                    "device": str(device),
                    "algorithm": "Stable Goal-Focused Independent PPO",
                    "learning_rate": 1e-4,
                    "goal_reward": 10.0,
                    "distance_reward_scale": 0.2,
                    "max_stationary_penalty": -0.05
                },
                tags=["stable", "goal-focused", "multi-agent", "ppo", "multigrid"]
            )
            print("âœ… WandBæ—¥å¿—å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸ WandBåˆå§‹åŒ–å¤±è´¥: {e}")
            use_wandb = False
    else:
        use_wandb = False
        print("ğŸ“ ä½¿ç”¨æœ¬åœ°æ—¥å¿—")
    
    # åˆ›å»ºç¨³å®šæ§åˆ¶å™¨
    controller = StablePPOController(
        env_name, n_agents, device, 
        lr=1e-4,  # é™ä½å­¦ä¹ ç‡
        n_parallel_envs=n_parallel_envs
    )
    
    # è®­ç»ƒæŒ‡æ ‡
    episode_rewards = []
    collective_rewards = []
    best_collective_reward = float('-inf')
    total_goal_touches = 0
    
    print(f"\nğŸ›¡ï¸ å¼€å§‹ç¨³å®šç‰ˆè®­ç»ƒ...")
    start_time = time.time()
    
    for episode in range(n_episodes):
        episode_start = time.time()
        
        # æ”¶é›†ç»éªŒ
        buffers, rollout_goal_touches = controller.collect_rollout(n_steps)
        total_goal_touches += rollout_goal_touches
        
        # æ›´æ–°æ™ºèƒ½ä½“
        update_metrics = controller.update_all_agents()
        
        # è®¡ç®—å¥–åŠ±
        agent_rewards = []
        for i in range(n_agents):
            agent_reward = sum(buffers[i]['rewards'])
            agent_rewards.append(agent_reward)
        
        collective_reward = sum(agent_rewards)
        episode_rewards.append(agent_rewards)
        collective_rewards.append(collective_reward)
        
        # æ£€æŸ¥è®­ç»ƒå¥åº·çŠ¶å†µ
        training_healthy = controller.check_training_health(collective_reward, episode)
        if not training_healthy and episode > 1000:
            print("ğŸš¨ æ£€æµ‹åˆ°è®­ç»ƒå¼‚å¸¸ï¼Œå»ºè®®åœæ­¢å¹¶æ£€æŸ¥å‚æ•°ï¼")
            break
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if collective_reward > best_collective_reward:
            best_collective_reward = collective_reward
            controller.save_models(f"models6/best_{experiment_name}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        episode_time = time.time() - episode_start
        
        if len(collective_rewards) >= 100:
            avg_collective = np.mean(collective_rewards[-100:])
            avg_individual = np.mean([np.mean([ep[i] for ep in episode_rewards[-100:]]) for i in range(n_agents)])
        else:
            avg_collective = collective_reward
            avg_individual = np.mean(agent_rewards)
        
        # WandBæ—¥å¿—
        if use_wandb:
            log_dict = {
                "episode": episode,
                "collective_reward": collective_reward,
                "avg_collective_reward_100": avg_collective,
                "avg_individual_reward_100": avg_individual,
                "best_collective_reward": best_collective_reward,
                "episode_time": episode_time,
                "total_samples": (episode + 1) * n_steps * n_parallel_envs,
                "goal_touches_total": total_goal_touches,
                "goal_touches_episode": rollout_goal_touches,
                "avg_loss": update_metrics.get('avg_loss', 0),
                "avg_grad_norm": update_metrics.get('avg_grad_norm', 0),
                "training_healthy": training_healthy
            }
            
            # æ·»åŠ ä¸ªä½“æ™ºèƒ½ä½“å¥–åŠ±
            for i in range(n_agents):
                log_dict[f"agent_{i}_reward"] = agent_rewards[i]
            
            wandb.log(log_dict)
        
        # æ§åˆ¶å°è¾“å‡º - å¢åŠ è°ƒè¯•ä¿¡æ¯
        if episode % 50 == 0:
            total_time = time.time() - start_time
            eps_per_hour = episode * 3600 / total_time if total_time > 0 else 0
            
            print(f"Episode {episode:6d} | "
                  f"é›†ä½“å¥–åŠ±: {avg_collective:8.2f} | "
                  f"ä¸ªä½“å¹³å‡: {avg_individual:8.2f} | "
                  f"æœ€ä½³: {best_collective_reward:8.2f} | "
                  f"è§¦ç¢°ç›®æ ‡: {rollout_goal_touches:2d} | "
                  f"æ€»è§¦ç¢°: {total_goal_touches:4d} | "
                  f"æ¢¯åº¦: {update_metrics.get('avg_grad_norm', 0):.3f} | "
                  f"é€Ÿåº¦: {eps_per_hour:.1f} ep/h")
            
            if not training_healthy:
                print("        ğŸš¨ è®­ç»ƒå¥åº·çŠ¶å†µå¼‚å¸¸ï¼")
        
        # å®šæœŸä¿å­˜
        if episode % 10000 == 0 and episode > 0:
            controller.save_models(f"models6/{experiment_name}_ep{episode}")
    
    # æœ€ç»ˆä¿å­˜
    controller.save_models(f"models6/{experiment_name}_final")
    controller.close_envs()
    
    # æœ€ç»ˆç»“æœ
    total_time = time.time() - start_time
    
    print(f"\nğŸ‰ ç¨³å®šç‰ˆè®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"â±ï¸ æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³é›†ä½“å¥–åŠ±: {best_collective_reward:.2f}")
    print(f"ğŸ“ˆ æœ€ç»ˆ100è½®å¹³å‡: {np.mean(collective_rewards[-100:]):.2f}")
    print(f"ğŸ¯ æ€»è§¦ç¢°ç›®æ ‡æ¬¡æ•°: {total_goal_touches}")
    print(f"ğŸ“Š å¹³å‡æ¯è½®è§¦ç¢°: {total_goal_touches/len(episode_rewards):.2f}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'episode_rewards': episode_rewards,
        'collective_rewards': collective_rewards,
        'best_collective_reward': best_collective_reward,
        'total_goal_touches': total_goal_touches,
        'total_time_hours': total_time / 3600,
        'config': {
            'env_name': env_name,
            'n_agents': n_agents,
            'n_episodes': len(episode_rewards),
            'experiment_name': experiment_name,
            'learning_rate': 1e-4,
            'stable_features': [
                'reduced_learning_rate',
                'stable_reward_shaping',
                'gradient_monitoring',
                'training_health_check'
            ]
        }
    }
    
    os.makedirs('models6', exist_ok=True)
    with open(f"models6/results_{experiment_name}.json", 'w') as f:
        json.dump(results, f)
    
    if use_wandb:
        wandb.finish()
    
    return controller, episode_rewards, collective_rewards, experiment_name


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="v4ç¨³å®šç‰ˆç›®æ ‡å¯¼å‘å¤šæ™ºèƒ½ä½“PPO")
    parser.add_argument("--env", type=str, default="MultiGrid-Cluttered-Fixed-15x15", 
                        help="ç¯å¢ƒåç§°")
    parser.add_argument("--episodes", type=int, default=50000, 
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--parallel-envs", type=int, default=4, 
                        help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--steps", type=int, default=128, 
                        help="æ¯æ¬¡rolloutçš„æ­¥æ•°")
    parser.add_argument("--no-wandb", action="store_true", 
                        help="ç¦ç”¨WandBæ—¥å¿—")
    parser.add_argument("--project", type=str, default="stable-goal-focused-ppo", 
                        help="WandBé¡¹ç›®åç§°")
    parser.add_argument("--name", type=str, default=None, 
                        help="å®éªŒåç§°")
    
    args = parser.parse_args()
    
    print("ğŸ›¡ï¸ å¯åŠ¨ç¨³å®šç‰ˆç›®æ ‡å¯¼å‘å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ...")
    print("ğŸ”§ ä¸»è¦ä¿®å¤:")
    print("   âœ… å­¦ä¹ ç‡é™ä½é˜²æ­¢å‚æ•°æ›´æ–°è¿‡å¤§")
    print("   âœ… å¥–åŠ±è®¾è®¡é¿å…è´Ÿå€¼ç´¯ç§¯")
    print("   âœ… æ¢¯åº¦ç›‘æ§é˜²æ­¢è®­ç»ƒå´©æºƒ")
    print("   âœ… æ—©åœæœºåˆ¶ä¿æŠ¤è®­ç»ƒè¿‡ç¨‹")
    print("="*70)
    
    controller, episode_rewards, collective_rewards, experiment_name = train_stable_ppo(
        env_name=args.env,
        n_episodes=args.episodes,
        n_steps=args.steps,
        n_parallel_envs=args.parallel_envs,
        use_wandb=not args.no_wandb,
        project_name=args.project,
        experiment_name=args.name
    )
    
    print(f"\nâœ… ç¨³å®šç‰ˆè®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: models6/{experiment_name}_final")
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: models6/results_{experiment_name}.json")